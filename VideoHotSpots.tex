%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Augmenting TV Newscasts via Entity Expansion  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{llncs}

\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}

\usepackage{makeidx}  % allows for indexgeneration
\usepackage[hyphens]{url}
\usepackage{textcomp}
\usepackage{color}
\usepackage{listings}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\setcounter{MaxMatrixCols}{20}
\usepackage{pbox}
\usepackage{amsfonts}


% listing styles
\lstset{numbers=left, numberstyle=\tiny,basicstyle=\ttfamily\scriptsize, tabsize=2, keywordstyle=\underbar, stringstyle=\small, backgroundcolor=\color[gray]{0.94}, framexleftmargin=2pt}
\lstdefinestyle{rdfa}{numberblanklines=true, morekeywords={}}



\begin{document}
\frontmatter          % for the preliminaries
\pagestyle{headings}  % switches on printing of running heads
\mainmatter              % start of the contributions

\title{Detecting and Displaying Hot Spots in Web Videos}
\author{Jos\'e Luis Redondo Garc\'ia\inst{1}, Mariella Sabatino\inst{1}, Pasquale Lisena\inst{1}, Rapha\"el Troncy\inst{1}}
\institute{
EURECOM, Sophia Antipolis, France, \\
\email{\{redondo, mariella.sabatino, pasquale.lisena, raphael.troncy\}@eurecom.fr}
}


\maketitle              % typeset the title of the contribution

%%%%%%%%%%%%%%%%%%
%%%  Abstract  %%%
%%%%%%%%%%%%%%%%%%

\begin{abstract}
We present an approach that leverages on visual analysis techniques and the knowledge present on the Web for identifying relevant fragments (called hot spots) inside a video from the Web, in order to promote the consumption of media resources at a higher level of granularity. 

Summary of the approach here. TED talks.

An online demo of the proposed solution is available at \url{http://linkedtv.eurecom.fr/mediafragmentplayer}.

\keywords{Video Annotation, Entity Expansion, News Enrichment}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  1. Introduction  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Today people consume all kind of audiovisual content on a daily basic. From breaking news to satiric videos passing by a tutorial on how to cook that wonderful meal, we are constantly bombarded with all kind of multimedia documents. In this media-overloaded scenario it becomes hardly complicated for us to decide if a candidate video is really worth to be watch, or which are particularly the fragments/s that can be potentially interesting without having to watch the entire video.

Some studies made over media entertainment streaming services~\cite{Yu2006} reveal that the majority of partial content views (52.55\%) are terminated by the user within the first 10 minutes, and about a 37\% of these sessions do not last past the first ﬁve minutes. This phenomena is even more evident when it comes to the the Web ecosystem \footnote{\fontsize{8pt}{1em}\selectfont \url{http://thenextweb.com/socialmedia/2014/05/02/optimal-length-video-marketing-content-short-possible/}}. In practice, it is difficult and time consuming to manually gather video insights that (1) give the viewers a fair understanding about what the video is talking about and (2) allow to easily visualize which fragments in particular are illustrating the main topics. Our research tackles this inconvenience by proposing a set of automatically annotated media fragments, called hot spots, which intend to highlight the main ideas of the video and make easier for the user to decide which fragment can be relevant for him to watch or share.

%Segmentation of videos
The challenge of video segmentation has been addressed by many previous research approaches. Some of them rely exclusively in visual and low-level features like color histograms or visual concept detection clustering operations~\cite{snoek2005multimodal}. On the other hand, there are some pure text–based implementations which leverage in the transcripts and written annotations that goes together with the video, like for example~\cite{chang1992image}. A special variety of the latter tries to go further and study the semantic behind the text by identifying relevant concepts and linking them to a taxonomy, like in~\cite{de2013ghent}. Finally, there are some initiatives that combine different kinds of techniques~\cite{chang2005combining} in order to keep the best of each. Our demo fits into this last category, with the added value of leveraging on the Web: it is applicable over online videos and it relies on the Web knowledge in order to analyze and annotate the content itself. 

% Semantic
Concerning the multimedia annotation the literature covers a wide range of different analysis techniques~\cite{ballan2011event}. One of the main approaches consists on running Named Entity Recognition (NER) over the textual information attached to particular video fragment. Those techniques are an essential component within the Information Extraction field that focus on: identifying atomic information units in texts, named entities; classifying entities into predefined categories (also called context types) and linking to real world objects using web identifiers (Named Entity Disambiguation). A growing number of APIs provide such a service, like AlchemyAPI\footnote{\fontsize{8pt}{1em}\selectfont \url{http://www.alchemyapi.com/}} or DBpedia Spotlight\footnote{\fontsize{8pt}{1em}\selectfont \url{http://spotlight.dbpedia.org/}}. If the textual information attached to a video contains temporal references (e.g. subtitles), it is possible to align the entities with the time when they appear in the video. In this line, Yunjia et al.~\cite{yunjia2013} have probed that named entity recognition techniques applied on video subtitles can produce good results for video classification. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Generating and Displaying Hot Spots in Web Videos    %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Generating and Displaying Hot Spots in Web Videos}
\label{sec:hotspots}

This demo implements a multimodal algorithm for detecting key fragments in a set of 1681 TED talks and annotating them in order to have a quick overview of which are the main topics involved and watch or share the specific parts of the media content which talk about those main ideas. In this section we unveil the details of this approach, specially in what concerns the segmentation of the video, the annotation of the obtained fragments, the selection of the hot spots, and the summarization of the main topics inside them.

%%%  Media Fragments Generation %%%
\subsection{Video Segmentation}
\label{sec:fragmentsgeneration}

In first place we perform a video segmentation based only in pure visual features. The resultant fragments, called shots, represent the continuous footage or sequence between two edits or cuts\footnote{\fontsize{8pt}{1em}\selectfont \url{http://en.wikipedia.org/wiki/Shot_(filmmaking)}} and can be consequently considered like the minimum visually-consistent video chunks. The algorithm used in our approach~\cite{sidiropoulos2011temporal} detects two kind of transitions between successive shots of the video: abrupt, where one frame belongs to a shot and the following frame belongs to the next shot, or gradual, when in the middle there are some spatial-chromatic video production effects which gradually replace one shot by another.   

However, those shots are too small when it comes to semantic consistency. A visual change in the frame flow does not necessary reveal a disruption in what is being told at that particular time of the video. We introduce then the notion of Chapters for naming chunks which illustrate particular topics inside the entire video context. In order to obtain such fragments we have leveraged in some marks embedded in the available video transcripts and not currently exploited by the TED portal, which indicate the start of a new paragraph. According to their definition \footnote{\fontsize{8pt}{1em}\selectfont \url{http://en.wikipedia.org/wiki/Paragraph}}, paragraphs are self-contained units of a discourse dealing with a particular point or idea, which is exactly what we are looking for at this point. Mapping those textual boundaries to the temporal references of the corresponding starting and ending subtitle blocks, we obtain the desired chapters. 

In a last step semantic fragments are combined with visual shots for keeping the best of both approaches. In particular we extend chapters back and forward in time in order to include comprise shots. This way we end up having semantically independent segments with have visually consistent borders at the same time.

%%%  Annotating Web Videos %%%
\subsection{Annotating Web Videos}
\label{sec:videoannotation}

The 1681 TED has subtitles available. 

Semantic clues: Named Entity Extraction and topic extraction

Confidence score from the Entity Extraction and topic extraction.
Temporal references allow to map every relevant topic

Result: list of fragments annotated

figure?


\subsection{Hot Spots Generation  MF Player}
\label{sec:hotspots}

  Clustering algorithm: Accumulative merging of temporally consecutive Chapters.
  
  Comparison function: frequency of the topic x relevance of the topic. The operation finished when there are not new merging operations.
  
  The number of fragments is still high. We want to filter then to obtain what we call the hot spots.

	Ranking of Chapters via Annotations. (Using Topics + Chapters) Inverse of the duration. (Introducing the temporal dimension)
  
 Summarizing chapter.
  This will be shown as a teaser of the fragment content
  
 
%%%  Use Case: TED talks %%%
\subsubsection{Displaying Hot spots in TED Talks demo}
\label{sec:usecase}

UI Description. (According to the pictures)

Connection Backend-Frontend via REST services.

%%%%%%%%%%%%%%%%%%%%%%%
%%%  4. Discussion  %%%
%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{sec:discussion}

Leverage on Web.
Easy compsuption
Nice graphic interface.
Future: Hyperlinking.
Future: Evaluantion

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Acknowledgments  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgments}
This work was partially supported by the European Union's 7th Framework Programme via the project LinkedTV (GA 287911).

%%%%%%%%%%%%%%%%%%%%%%
%%%  Bibliography  %%%
%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{abbrv}
\bibliography{VideoHotSpots}

\end{document}
